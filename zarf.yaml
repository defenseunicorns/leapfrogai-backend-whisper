kind: ZarfPackageConfig
metadata:
  name: "###ZARF_PKG_TMPL_NAME###"
  version: "###ZARF_PKG_TMPL_IMAGE_VERSION###"
  description: >
    whisper model

variables:
  - name: GPU_ENABLED
    description: Optionally turn on GPU inferencing for environments with capable CUDA devices (true/false)
    default: false
    prompt: true

constants:
  - name: IMAGE_VERSION
    value: "###ZARF_PKG_TMPL_IMAGE_VERSION###"
  - name: NAME
    value: "###ZARF_PKG_TMPL_NAME###"

components:
  - name: import-model
    required: true
    import:
      name: model
      url: oci://ghcr.io/defenseunicorns/packages/leapfrogai/leapfrogai-model:0.4.0-skeleton
    actions:
      onDeploy:
        defaults:
          shell:
            linux: sh
        after:
          - cmd: |
              if [ "${ZARF_VAR_GPU_ENABLED}" = "true" ]; then kubectl patch deployment whisper-model-whisper -n leapfrogai --patch '{"spec":{"template":{"spec":{"containers":[{"name":"leapfrogai-model","resources":{"limits":{"cpu":"1","memory":"20Gi","nvidia.com/gpu":"1"},"requests":{"cpu":"1","memory":"4Gi","nvidia.com/gpu":"1"}}}]}}}}'; fi
